from scripts.base_generation import *
from generators.two_stroke_words import two_strokes
from toolz import functoolz
from functools import partial

cores = 8

my_target_file = "../input_dictionaries/40_autogenerated_joukahainen_double.yaml"

def strokefy(inflection_forms, ignore_words):
    def parse(w):
        try:
            return two_strokes(w)
        except ParseError:
            pass

    return [parse(w) for w in inflection_forms
            if w not in ignore_words]

def create_strokes_for_words(words_raw, ignore_words):
    strokes = functoolz.thread_last(words_raw,
                                    (map, inflected_forms),
                                    (map, lambda words: strokefy(words,
                                                                 ignore_words)),
                                    # realize the lazy sequence
                                    list)
    print("Processed {} strokes.".format(len(words_raw)))
    return strokes

def main():
    (chunks, ignore_words) = wordlist_chunks(cores, my_target_file)

    with Pool(cores) as pool:
        # "map" step
        def do_create(words):
            return create_strokes_for_words(words, ignore_words)
        do_create = partial(create_strokes_for_words, ignore_words = ignore_words)

        strokes = pool.map(do_create, chunks)

        # "reduce" step
        word_stroke_dict = flatten_dictify_matched(strokes)

        write_to_yaml_file(my_target_file, word_stroke_dict)
        return 0

if __name__ == '__main__':
    sys.exit(main())
