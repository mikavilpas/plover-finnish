from scripts.base_generation import *
from generators.two_stroke_words import two_strokes

cores = 8

def strokefy(inflection_forms):
    def parse(w):
        try:
            return two_strokes.parse(w)
        except ParseError:
            pass

    return [[w, parse(w)]
            for w in inflection_forms]

def create_strokes_for_words(words_raw):
    strokes = functoolz.thread_last(words_raw,
                                    (map, inflected_forms),
                                    (map, strokefy),
                                    # realize the lazy sequence
                                    list)
    print("Processed {} strokes.".format(len(words_raw)))
    return strokes

def main():
    chunks = wordlist_chunks(cores)
    with Pool(cores) as pool:
        # "map" step
        strokes = pool.map(create_strokes_for_words, chunks)

        # "reduce" step
        word_stroke_dict = flatten_dictify_matched(strokes)

        target_file = "../input_dictionaries/autogenerated_joukahainen_double.yaml"
        write_to_yaml_file(target_file, word_stroke_dict)
        return 0

if __name__ == '__main__':
    sys.exit(main())
